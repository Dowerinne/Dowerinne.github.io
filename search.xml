<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Transformer和多头注意力机制</title>
      <link href="/2024/12/10/Transformer%E5%92%8C%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/12/10/Transformer%E5%92%8C%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="多头自注意力机制与Transformer"><a href="#多头自注意力机制与Transformer" class="headerlink" title="多头自注意力机制与Transformer"></a>多头自注意力机制与Transformer</h1><p>2024.12.06整理</p><h4 id="自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）"><a href="#自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）" class="headerlink" title="自注意力机制（计算过程见：https://zhuanlan.zhihu.com/p/338817680 3.2节与3.3节）"></a>自注意力机制（计算过程见：<a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/338817680</a> 3.2节与3.3节）</h4><p>有$M$个特征（或者单词）   （又叫seq_len: 句子长度,即单词数量）</p><p>每个单词用一个$d$维向量表示，记作矩阵<script type="math/tex">X_{M*d}</script>，矩阵有$M$行，<strong>每行对应一个特征/单词</strong>，每行是$d$维。</p><p>Self-Attention 的输入是矩阵$X$，则可以使用线性变换矩阵$W_Q,W_K,W_V$计算得到$Q,K,V$矩阵。</p><p>$W_Q,W_K,W_V$的维度是<script type="math/tex">d*d^{\prime}</script>，得到的$Q,K,V$矩阵的维度是<script type="math/tex">M*d'</script>。 <strong>$X, Q, K, V$的每一行都对应一个单词。</strong></p><p>矩阵乘法：</p><p>$Q=X×W_Q$</p><p>$K=X×W_K$</p><p>$V=X×W_V$</p><p>然后计算$Score(Q,K^T)$，即矩阵相乘$QK^T$，维度是$M*M$，元素$a_{ij}$代表第$i$个单词和第$j$个单词之间的关联程度（attention强度/权重/attention系数/相似性）。</p><p>（为了防止$a_{ij}$过大，所有的$a_{ij}$都要除以$\sqrt{d’}$）</p><p>再对$QK^T$的每一行做$softmax$，使得每行各元素之和都为1.</p><p>得到新的维度为$M*M$的矩阵$softmax(QK^T)$。</p><p>再和矩阵$V$进行元素相乘，得到最终的输出矩阵$Z$，$Z=softmax(QK^T) × V$       </p><p>$Z$的维度是$M*d’$</p><p><strong>矩阵$X,V,Z$的每一行都是对每个单词的向量表示，都有$M$行，因此该过程简单来讲就是$X→V→Z$，</strong></p><p>得到最终的矩阵$Z$，<font color='red'>$Z$的每行是作为对每个单词的新向量表示。</font></p><font color='red'>每个单词的新向量，都是融合了其它单词信息的新向量。而$X$的每行是没有融合其他单词信息的旧向量。</font><p>计算公式写成：</p><p>自注意力机制的输出为矩阵$Z$：</p><script type="math/tex; mode=display">Z=Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d'}})V</script><h4 id="多头自注意力机制"><a href="#多头自注意力机制" class="headerlink" title="多头自注意力机制"></a>多头自注意力机制</h4><p>（一组头：就是一组$Q \ K \ V$矩阵，多头有多组$Q \ K \ V$矩阵，从而得到不同的输出矩阵$Z_i$）</p><p>【不同的头又叫不同的“子空间”，<strong>头=子空间</strong>，可理解为不同的<strong>角度</strong>】</p><p>【如果想从多个角度看，可以用多个头，即换不同的矩阵$W_Q,W_K,W_V$得到不同的$Q,K,V$，然后得到不同的输出矩阵$Z_i$】</p><p>多头自注意力机制<strong>Multi-Head Attention</strong>，是由多个自注意力机制<strong>Self-Attention</strong>组成的。</p><p>Multi-Head Attention 包含多个 Self-Attention 层，</p><p>首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>$Z_i$</strong>。</p><p>比如当 h=8 时，此时会得到 8 个输出矩阵<strong>$Z_i$</strong>。</p><p>得到 8 个输出矩阵$Z_1$~$Z_8$之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p><p>（每个$Z_i$的维度是<script type="math/tex">M*d'</script>，与$X$的维度不同；<strong>而多头自注意力机制的最终输出$Z$的维度是<script type="math/tex">M*d</script>，与$X$的维度相同</strong>。）</p><h1 id="Transformer其他细节："><a href="#Transformer其他细节：" class="headerlink" title="Transformer其他细节："></a>Transformer其他细节：</h1><p>见下面，写的易懂，要多看！</p><p><a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/338817680</a></p><p>补充笔记：</p><p>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding！</p><p>$PE_{pos,2i}，PE_{pos,2i+1}$，这里的$2i,2i+1$是维度索引，即分量，是在计算奇数编号和偶数编号的<strong>分量</strong>的值。</p><h5 id="层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别："><a href="#层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别：" class="headerlink" title="层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别："></a>层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别：</h5><p>假设输入矩阵为$X_{n*d}$，</p><p>n 是批量大小（batch size），代表有n个单词/物品，<strong>一行对应一个</strong>；</p><p>d 是每个单词/物品的特征数量。</p><p>Layer Normalization 的步骤如下：</p><ol><li><p><strong>计算均值</strong>： <strong>对每个样本（行）</strong>计算特征的均值。对于第$i$个样本$\mathbf{x}_i = (x_{i1}, x_{i2}, …, x_{id})$，均值是：</p><script type="math/tex; mode=display">\mu_i = \frac{1}{d} \sum_{j=1}^{d} x_{ij}</script></li><li><p><strong>计算方差</strong>： 对每个样本计算特征的方差：</p><script type="math/tex; mode=display">\sigma_i^2 = \frac{1}{d} \sum_{j=1}^{d} (x_{ij} - \mu_i)^2</script></li><li><p><strong>归一化</strong>： 使用均值和方差对每个特征进行归一化：</p><script type="math/tex; mode=display">\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}</script><p>其中 $\epsilon$ 是一个非常小的常数，用于防止除零错误。</p></li><li><p><strong>缩放和平移</strong>： 在归一化后，Layer Normalization 会引入两个可学习的参数 $\gamma$ 和 $\beta$，分别用于缩放和平移。最终输出是：</p><script type="math/tex; mode=display">y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j</script><p>其中 $\gamma_j$ 和 $\beta_j$是为每个特征（每个列）学习到的参数。</p></li></ol><p>总结：</p><p>Layer Normalization 对每个样本的特征进行归一化。</p><p>它不像 Batch Normalization 依赖于批量数$n$，而是<strong>对每个样本单独归一化</strong>，适用于处理序列数据（如RNN、Transformer等），尤其在批量大小较小或单个样本处理时。</p><p>批归一化的步骤：</p><ol><li><p><strong>计算批次均值</strong>： 对整个批次的<strong>每个特征维度（列）</strong>计算均值，即计算<strong>每一列（特征）在批次中所有样本的均值</strong>。对于第$j$个特征维度，它的批次均值是：</p><script type="math/tex; mode=display">\mu_j = \frac{1}{n} \sum_{i=1}^{n} x_{ij}</script><p>其中，$x_{ij}$是第 $i$个样本在第$j$个特征维度上的值。</p></li><li><p><strong>计算批次方差</strong>： 对整个批次的每个特征维度（列）计算方差，即计算每一列（特征）在批次中所有样本的方差。对于第$j$个特征维度，它的批次方差是：</p><script type="math/tex; mode=display">\sigma_j^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{ij} - \mu_j)^2</script><p>这里的$\sigma_j^2$是第$j$个特征维度在批次中的方差。</p></li><li><p><strong>归一化</strong>： 使用计算得到的均值$\mu_j$和方差 $\sigma_j^2$对每个样本的每个特征进行归一化处理。对于第$i$个样本在第$j$个特征维度上的归一化值：</p><script type="math/tex; mode=display">\hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}</script><p>这里$\epsilon$ 是一个很小的常数，用来防止方差为零时出现除零错误。</p></li><li><p><strong>缩放和平移（可选）</strong>： 批归一化通常会引入两个可学习的参数：$\gamma_j$和$\beta_j$，分别用于缩放和偏移归一化后的值。这样做的目的是让网络有更大的表达能力，因为在训练过程中，经过归一化的输出可以通过这些参数重新调整回原始尺度。</p><script type="math/tex; mode=display">y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j</script><p>其中，$\gamma_j$是缩放因子，$\beta_j$是偏移因子，通常会初始化为1和0。</p></li></ol><p>批归一化通常在卷积神经网络（CNN）和全连接层网络（FCN）中使用。</p><p><strong>层归一化</strong>：是在每个样本的特征维度上进行归一化，每个样本独立归一化。<strong>每行进行归一化、</strong></p><p><strong>批归一化</strong>：是在整个批次上对每个特征维度进行归一化，考虑了批次中的所有样本。<strong>每列进行归一化。</strong></p><p>每个Decoder Block：</p><ul><li>包含两个 Multi-Head Attention 层。</li><li>第一个 Multi-Head Attention 层采用了 Masked 操作：<ul><li>在计算得到$QK^T$，对$QK^T$的每一行做$softmax$之前，</li><li>先使用<strong>Mask</strong>矩阵，将$QK^T$变成<strong>Mask</strong> $QK^T$，</li><li>再对<strong>Mask</strong> $QK^T$的每一行做$softmax$。</li></ul></li><li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算得出，而<strong>Q</strong>使用上一个 Decoder block 的输出计算得出。</li></ul><p>Encoder和Decoder的输入输出：</p><p>Encoder的输入：源文本的单词表示矩阵$X_{n,d}$，  如“我 有 一只 猫”，共4个单词</p><p>Encoder的输出：信息编码矩阵$C_{n,d}$，<strong>与$X$的维度一致</strong>，<script type="math/tex">（n=seq\_len,d=emd\_size）</script></p><p>Decoder的输入：目标文本的单词表示矩阵$X_{n,d}$，如”&lt;Begin&gt; I have a cat”，共5个单词</p><p>（Encoder的输出矩阵$C$会输入到Decoder的第二个$Multi-Head \ Attention$中）</p><p>Decoder的输出：预测的单词序列，如“I have a cat &lt;end&gt;”。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建记录_20241208</title>
      <link href="/2024/12/09/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95-20241208/"/>
      <url>/2024/12/09/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95-20241208/</url>
      
        <content type="html"><![CDATA[<p>2024.12.08下午，终于决定建个博客，12.09初步弄好了。</p><h1 id="2024-12-08-博客搭建记录整理"><a href="#2024-12-08-博客搭建记录整理" class="headerlink" title="2024.12.08 博客搭建记录整理"></a>2024.12.08 博客搭建记录整理</h1><p>方案：github pages + hexo</p><h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><p>0.Github上新建<code>用户名.github.io</code>仓库</p><p>1.安装Git，生成SSH，添加到Github的SSH keys</p><p>2.安装node.js和npm</p><p>2.5 修改npm资源获取地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry http:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure><p>3.安装hexo</p><p>4.后续步骤见：<a href="https://blog.csdn.net/weixin_44763569/article/details/106435118" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44763569/article/details/106435118</a></p><p>5.主题选择：<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a></p><p>我选的是<code>A-Snail</code>主题，安装教程/【配置】见：<a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">https://github.com/dusign/hexo-theme-snail</a></p><p>6.修改<code>F:\myBlog_241208\blog\_csonfig.yml</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:Dowerinne/Dowerinne.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure><p>7.<code>npm install hexo-deployer-git --save</code></p><p>8.<code>npm install hexo-util@1.9.1</code>      【重要，必须指定成这个版本，防报错】</p><p>9.修改引擎bug，修改文件<code>/node_modules\kramed\lib\rules\inline.js</code>中<code>escape</code>和<code>em</code>两行，具体修改如下，源自<a href="https://www.jianshu.com/p/9b9c241146bc" target="_blank" rel="noopener">https://www.jianshu.com/p/9b9c241146bc</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;  escape: &#x2F;^\\([\\&#96;*&#123;&#125;\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">  escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,</span><br><span class="line">  </span><br><span class="line">&#x2F;&#x2F;  em: &#x2F;^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br><span class="line">  em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure><p>我的本地文件夹：<code>F:\myBlog_241208\blog</code>，在该目录下打开<code>cmd 或 Git Bash</code>输入命令。</p><h4 id="A-Snail主题安装与配置"><a href="#A-Snail主题安装与配置" class="headerlink" title="A-Snail主题安装与配置"></a><code>A-Snail</code>主题安装与配置</h4><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">https://github.com/dusign/hexo-theme-snail</a></p><p>安装：见Quick Start一节，先后安装了Theme，Mathjax，WordCount</p><p>配置：见Configuration一节【重要】，是修改<code>F:\myBlog_241208\blog\_config.yml</code>。 <font color='red'>未完待续，继续弄</font></p><h4 id="配置记录："><a href="#配置记录：" class="headerlink" title="配置记录："></a>配置记录：</h4><h5 id="1-修改header-img：图片放入img-文件夹中。"><a href="#1-修改header-img：图片放入img-文件夹中。" class="headerlink" title="1.修改header_img：图片放入img/文件夹中。"></a>1.修改header_img：图片放入<code>img/</code>文件夹中。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Site settings</span><br><span class="line">SEOTitle: Dowerinne&#39;s blog</span><br><span class="line">email: </span><br><span class="line">description: &quot;A hexo theme&quot;</span><br><span class="line">keyword: &quot;hexo-theme-snail&quot;</span><br><span class="line">header-img: img&#x2F;header_img&#x2F;sunset.jpg</span><br><span class="line">signature: false #show signature</span><br><span class="line">signature-img: img&#x2F;signature&#x2F;dusign.png</span><br></pre></td></tr></table></figure><p>注意这里的<code>img/</code>文件夹是<code>F:\myBlog_241208\blog\source\img</code>，并不是<code>F:\myBlog_241208\blog\img</code>！！</p><h5 id="2-每篇文章的header-img："><a href="#2-每篇文章的header-img：" class="headerlink" title="2.每篇文章的header_img："></a>2.每篇文章的header_img：</h5><p>图片是放在<code>F:\myBlog_241208\blog\source\img</code>文件夹中！</p><p>修改每个md文件最上方的<code>header-img: ../../../../img/default.jpg</code>，把default.jpg换成目标图片即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">路径解释：..&#x2F;..&#x2F;..&#x2F;..&#x2F;img&#x2F;wallpaper_01.jpg</span><br><span class="line">并不是从md当前的位置出发 F:\myBlog_241208\blog\source\_posts，</span><br><span class="line">而是从这个位置出发：F:\myBlog_241208\blog\public\2024\12\09\博客搭建记录-20241208</span><br><span class="line"></span><br><span class="line">其他md以此类推。</span><br></pre></td></tr></table></figure><p>但是修改的时候，图片还是要放在<code>F:\myBlog_241208\blog\source\img</code>文件夹中，</p><p>而不是放到<code>F:\myBlog_241208\blog\public\img</code>里面。</p><h5 id="3-LaTeX-MarkDown数学公式相关："><a href="#3-LaTeX-MarkDown数学公式相关：" class="headerlink" title="3.LaTeX/MarkDown数学公式相关："></a>3.LaTeX/MarkDown数学公式相关：</h5><p>①如果md文档里有数学公式需要渲染，文档最上面加上<code>mathjax: true</code>，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">title: Transformer和多头注意力机制</span><br><span class="line">......</span><br><span class="line">date: 2024-12-10 23:16:05</span><br><span class="line">mathjax: true</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>②所有的公式都要写成<script type="math/tex">$...$$$，而不是</script>…<script type="math/tex">因为如果</script>…$$里面有<code>*</code>，而同一行又有加粗字体<code>**...**</code>，页面上渲染会有问题！！</p><p>或者是直接插入公式块。</p><p>③尖括号的渲染问题：如果想在文档中显示尖括号，即&lt; 和 &gt;，写成<code>&amp;lt;</code>和<code>&amp;gt;</code></p><h4 id="hexo命令整理：【待补充继续学习】-https-hexo-io"><a href="#hexo命令整理：【待补充继续学习】-https-hexo-io" class="headerlink" title="hexo命令整理：【待补充继续学习】 https://hexo.io/"></a>hexo命令整理：【<font color='red'>待补充继续学习</font>】 <a href="https://hexo.io/" target="_blank" rel="noopener">https://hexo.io/</a></h4><p>1.<code>hexo new &lt;articlename&gt;</code>：新建一篇博文，存放在<code>F:\myBlog_241208\blog\source\_posts\&lt;articlename&gt;.md</code>，都是markdown格式，用typora书写。</p><blockquote><p>所有博文都在<code>F:\myBlog_241208\blog\source\_posts\</code>目录下。</p></blockquote><p>2.发布/修改博文后更新：<code>hexo clean</code>   <code>hexo g -d</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>如果有报错，重新输入一遍命令，就没报错了。</p><p>3.本地查看页面效果：<code>hexo server</code>，浏览器中再打开<code>localhost:4000</code></p><h4 id="补充教程（供参考）："><a href="#补充教程（供参考）：" class="headerlink" title="补充教程（供参考）："></a>补充教程（供参考）：</h4><p><a href="https://saltyfishyjk.github.io/2022/01/28/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%9B%9E%E9%A1%BE%E6%A6%82%E8%A7%88/" target="_blank" rel="noopener">https://saltyfishyjk.github.io/2022/01/28/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%9B%9E%E9%A1%BE%E6%A6%82%E8%A7%88/</a></p><h4 id="过程中遇到的一些错误的解决方案："><a href="#过程中遇到的一些错误的解决方案：" class="headerlink" title="过程中遇到的一些错误的解决方案："></a>过程中遇到的一些错误的解决方案：</h4><p><a href="https://blog.csdn.net/hannikan/article/details/120838724" target="_blank" rel="noopener">npm : 无法加载文件 D:\nodejs\npm.ps1</a></p><p><a href="https://juejin.cn/post/7151669178381008910" target="_blank" rel="noopener">hexo : 无法加载文件\AppData\Roaming\npm\hexo.ps1</a></p><p><a href="https://blog.csdn.net/2301_81265915/article/details/144092365" target="_blank" rel="noopener">解决 git 报错 “fatal: unable to access ‘https://github.com/…/.git‘: Recv failure Connection was reset</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hello-blog</title>
      <link href="/2024/12/09/hello-blog/"/>
      <url>/2024/12/09/hello-blog/</url>
      
        <content type="html"><![CDATA[<p>2024.12.09 test</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
