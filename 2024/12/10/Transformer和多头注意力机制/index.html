<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="google-site-verification"
    content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI"
  />
  <meta name="baidu-site-verification" content="093lY4ziMu" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, user-scalable=no"
  />
  <meta name="description" content="A hexo theme" />
  <meta name="keyword" content="hexo-theme-snail" />
  <link rel="shortcut icon" href="/img/wx_icon.jpg" />
  <link
    rel="stylesheet"
    href="https://unpkg.com/@waline/client@v3/dist/waline.css"
  />
  <link rel="stylesheet" href="/css/walineRoot.css" />
  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
  <title>
     Transformer和多头注意力机制 - Dowerinne&#39;s blog 
  </title>

  <link
    rel="canonical"
    href="https://dowerinne.github.io/2024/12/10/Transformer和多头注意力机制/"
  />

  <!-- Bootstrap Core CSS -->
  
<link rel="stylesheet" href="/css/bootstrap.min.css">


  <!-- Custom CSS -->
   
<link rel="stylesheet" href="/css/dusign-dark.css">
 
<link rel="stylesheet" href="/css/dusign-common-dark.css">
 
<link rel="stylesheet" href="/css/font-awesome.css">
 
<link rel="stylesheet" href="/css/toc.css">
 

  <!-- Pygments Highlight CSS -->
  
<link rel="stylesheet" href="/css/highlight.css">
 
<link rel="stylesheet" href="/css/widget.css">
 
<link rel="stylesheet" href="/css/rocket.css">

  
<link rel="stylesheet" href="/css/signature.css">
 
<link rel="stylesheet" href="/css/fonts.googleapis.css">


  <link
    rel="stylesheet"
    href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"
  />

  <!-- photography -->
  
<link rel="stylesheet" href="/css/photography.css">


  <!-- ga & ba script hoook -->
  <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.5)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                        </div>
                        <h1>Transformer和多头注意力机制</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Dowerinne on
                            2024-12-10
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">4.2k</span> and
                                Reading Time <span class="post-count">16</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-dark.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-dark.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Dowerinne&#39;s blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/photography/">Photography</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Post Content -->
<article>
  <div class="container">
    <div class="row">
      <!-- Post Container -->
      <div
        class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 post-container"
      >
        <h1 id="多头自注意力机制与Transformer"><a href="#多头自注意力机制与Transformer" class="headerlink" title="多头自注意力机制与Transformer"></a>多头自注意力机制与Transformer</h1><p>2024.12.06整理</p>
<h4 id="自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）"><a href="#自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）" class="headerlink" title="自注意力机制（计算过程见：https://zhuanlan.zhihu.com/p/338817680 3.2节与3.3节）"></a>自注意力机制（计算过程见：<a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/338817680</a> 3.2节与3.3节）</h4><p>有$M$个特征（或者单词）   （又叫seq_len: 句子长度,即单词数量）</p>
<p>每个单词用一个$d$维向量表示，记作矩阵<script type="math/tex">X_{M*d}</script>，矩阵有$M$行，<strong>每行对应一个特征/单词</strong>，每行是$d$维。</p>
<p>Self-Attention 的输入是矩阵$X$，则可以使用线性变换矩阵$W_Q,W_K,W_V$计算得到$Q,K,V$矩阵。</p>
<p>$W_Q,W_K,W_V$的维度是<script type="math/tex">d*d^{\prime}</script>，得到的$Q,K,V$矩阵的维度是<script type="math/tex">M*d'</script>。 <strong>$X, Q, K, V$的每一行都对应一个单词。</strong></p>
<p>矩阵乘法：</p>
<p>$Q=X×W_Q$</p>
<p>$K=X×W_K$</p>
<p>$V=X×W_V$</p>
<p>然后计算$Score(Q,K^T)$，即矩阵相乘$QK^T$，维度是$M*M$，元素$a_{ij}$代表第$i$个单词和第$j$个单词之间的关联程度（attention强度/权重/attention系数/相似性）。</p>
<p>（为了防止$a_{ij}$过大，所有的$a_{ij}$都要除以$\sqrt{d’}$）</p>
<p>再对$QK^T$的每一行做$softmax$，使得每行各元素之和都为1.</p>
<p>得到新的维度为$M*M$的矩阵$softmax(QK^T)$。</p>
<p>再和矩阵$V$进行元素相乘，得到最终的输出矩阵$Z$，$Z=softmax(QK^T) × V$       </p>
<p>$Z$的维度是$M*d’$</p>
<p><strong>矩阵$X,V,Z$的每一行都是对每个单词的向量表示，都有$M$行，因此该过程简单来讲就是$X→V→Z$，</strong></p>
<p>得到最终的矩阵$Z$，<font color='red'>$Z$的每行是作为对每个单词的新向量表示。</font></p>
<font color='red'>每个单词的新向量，都是融合了其它单词信息的新向量。而$X$的每行是没有融合其他单词信息的旧向量。</font>

<p>计算公式写成：</p>
<p>自注意力机制的输出为矩阵$Z$：</p>
<script type="math/tex; mode=display">
Z=Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d'}})V</script><h4 id="多头自注意力机制"><a href="#多头自注意力机制" class="headerlink" title="多头自注意力机制"></a>多头自注意力机制</h4><p>（一组头：就是一组$Q \ K \ V$矩阵，多头有多组$Q \ K \ V$矩阵，从而得到不同的输出矩阵$Z_i$）</p>
<p>【不同的头又叫不同的“子空间”，<strong>头=子空间</strong>，可理解为不同的<strong>角度</strong>】</p>
<p>【如果想从多个角度看，可以用多个头，即换不同的矩阵$W_Q,W_K,W_V$得到不同的$Q,K,V$，然后得到不同的输出矩阵$Z_i$】</p>
<p>多头自注意力机制<strong>Multi-Head Attention</strong>，是由多个自注意力机制<strong>Self-Attention</strong>组成的。</p>
<p>Multi-Head Attention 包含多个 Self-Attention 层，</p>
<p>首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>$Z_i$</strong>。</p>
<p>比如当 h=8 时，此时会得到 8 个输出矩阵<strong>$Z_i$</strong>。</p>
<p>得到 8 个输出矩阵$Z_1$~$Z_8$之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p>
<p>（每个$Z_i$的维度是<script type="math/tex">M*d'</script>，与$X$的维度不同；<strong>而多头自注意力机制的最终输出$Z$的维度是<script type="math/tex">M*d</script>，与$X$的维度相同</strong>。）</p>
<h4 id="三种注意力机制（Encoder一种，Decoder两种）"><a href="#三种注意力机制（Encoder一种，Decoder两种）" class="headerlink" title="三种注意力机制（Encoder一种，Decoder两种）"></a>三种注意力机制（Encoder一种，Decoder两种）</h4><p>Encoder块的数量，Decoder块的数量：N_e  N_d 是超参数，可设置</p>
<p><img src="https://picx.zhimg.com/70/v2-7be8fe269991a236f000168291481c8b_1440w.image?source=172ae18b&amp;biz_tag=Post" alt="Transformer模型详解（图解最完整版）"></p>
<p>Encoder一种：每个Encoder块里面有，缩放点积自注意力</p>
<p>第一个Encoder块的输入：源序列<script type="math/tex">X_{M*d}</script> ，算出当前Encoder块的Q K V，输出是<script type="math/tex">Z_{M*d}</script></p>
<p>第2~n个Encoder块的输入：上一个Encoder块的输出<script type="math/tex">Z_{M*d}</script>，算出当前Encoder块的Q K V，再输出新的<script type="math/tex">Z_{M*d}</script></p>
<p>第n个Encoder块的输出：编码信息矩阵C，维度也是<script type="math/tex">C_{M*d}</script></p>
<p>Decoder两种：每个Decoder块里面有，掩码自注意力机制，交叉注意力机制</p>
<p>第一个Decoder块的输入：<strong>目标序列<script type="math/tex">X^{goal}_{M*d}</script></strong> 【不是源序列<script type="math/tex">X_{M*d}</script>】和C，输出是<script type="math/tex">Z_{M*d}</script></p>
<p>第2~n个Decoder块的输入：上一个Decoder块的输出<script type="math/tex">Z_{M*d}</script>和C，再输出新的<script type="math/tex">Z_{M*d}</script></p>
<p>第n个Decoder块的输出：<script type="math/tex">Z^{final}_{M*d}</script></p>
<h5 id="后续过程："><a href="#后续过程：" class="headerlink" title="后续过程："></a>后续过程：</h5><p>将<script type="math/tex">Z^{final}_{M*d}</script>通过一个线性层（<code>Linear</code>），维度从 <code>[M, d]</code> 变为 <code>[M, vocab_size]</code>。（M个单词，每个单词有vocab_size种可能）</p>
<p>现在，我们得到了一个矩阵，可以称为 <code>Logits</code>。它的每一行（共M行）对应目标序列中的一个位置，每一行的向量（长度为 <code>vocab_size</code>）代表了在该位置词汇表中所有单词的得分（logits）</p>
<p>对 <code>Logits</code> 矩阵的<strong>每一行</strong>独立地进行 Softmax 操作，得到 <code>Probs</code>矩阵，维度是<code>[M, vocab_size]</code>，但现在每一行都变成了一个概率分布（该行所有元素之和为1）。</p>
<p>对于 <code>Probs</code> 矩阵的<strong>每一行</strong>（即目标序列的每一个位置），选择概率值最大的那个元素所在的列索引。</p>
<p>得到一个包含 M 个索引值的向量，形状为 <code>[M]</code>。每个索引值对应词汇表中的一个单词。</p>
<p>将这 M 个索引值通过一个<strong>查找表</strong>，映射回词汇表中对应的实际单词。</p>
<p><strong>最终输出：</strong> 得到一个由 M 个单词组成的序列，这就是模型生成的整个目标句子（例如翻译结果）。</p>
<p><strong>上面是非自回归的过程！！Decoder在训练时是非自回归</strong>，推理时是自回归。</p>
<p>推理时：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left"><strong> Encoder（编码器侧）</strong></th>
<th><strong>Decoder（解码器侧）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>运行次数</strong></td>
<td style="text-align:left"><strong>只运行1次</strong></td>
<td><strong>运行M次</strong>（循环生成目标序列的每一个词）</td>
</tr>
<tr>
<td style="text-align:left"><strong>输入</strong></td>
<td style="text-align:left">完整的源序列 <code>[M_src, d]</code></td>
<td>不断增长的目标序列：<code>[1, d]</code> -&gt; <code>[2, d]</code> -&gt; … -&gt; <code>[M, d]</code></td>
</tr>
<tr>
<td style="text-align:left"><strong>输出</strong></td>
<td style="text-align:left">编码信息 <code>C [M_src, d]</code></td>
<td>下一个词的概率分布 <code>[vocab_size]</code></td>
</tr>
<tr>
<td style="text-align:left"><strong>工作状态</strong></td>
<td style="text-align:left"><strong>提前完成，结果被缓存</strong></td>
<td><strong>循环工作，串行生成</strong></td>
</tr>
</tbody>
</table>
</div>
<p>输入Encoder的源序列：可以理解为输入进LLM的prompt。</p>
<p>输入Decoder的目标序列：</p>
<ul>
<li><strong>训练时【非自回归】：</strong> 第一个Decoder块的输入是完整的<strong>目标序列</strong> <code>X_goal [M, d]</code>（并使用掩码确保并行计算时不泄露未来信息）。</li>
<li><strong>推理时【自回归】：</strong> 第一个Decoder块的输入是起始符 <code>[&lt;sos&gt;]</code>，形状为 <code>[1, d]</code>。<strong>每次生成一个新词，就将其拼接到当前输入序列的末尾</strong>，输入长度从 <code>[1, d]</code> -&gt; <code>[2, d]</code> -&gt; … -&gt; <code>[M, d]</code>，逐步增加，直到生成结束符。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">训练时（非自回归）</th>
<th style="text-align:left">推理时（自回归）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">一次处理整个序列 <code>[T, d_model]</code></td>
<td style="text-align:left">逐步生成，每次只处理到当前序列 <code>[当前长度, d_model]</code></td>
</tr>
<tr>
<td style="text-align:left">线性层输出 <code>[T, vocab_size]</code></td>
<td style="text-align:left"><strong>每次只取最后一个向量</strong>，维度是 <code>[vocab_size]</code></td>
</tr>
<tr>
<td style="text-align:left">得到T个概率分布，<strong>一次性</strong>选出T个词</td>
<td style="text-align:left">得到<strong>1个</strong>概率分布，<strong>只选1个</strong>词，然后循环</td>
</tr>
</tbody>
</table>
</div>
<p>所以，<strong>Decoder在每一步只生成一个词</strong>。它利用<strong>整个已生成的序列</strong>作为上下文，但只输出<strong>下一个词</strong>的概率分布。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left"><strong>训练 (Training)</strong></th>
<th style="text-align:left"><strong>推理 (Inference)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>模式</strong></td>
<td style="text-align:left"><strong>非自回归 (Parallel)</strong></td>
<td style="text-align:left"><strong>自回归 (Sequential)</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>输入</strong></td>
<td style="text-align:left">完整的目标序列（右移一位）</td>
<td style="text-align:left">模型自己之前生成的结果</td>
</tr>
<tr>
<td style="text-align:left"><strong>输出</strong></td>
<td style="text-align:left">整个序列的概率分布 <code>[T, vocab_size]</code></td>
<td style="text-align:left">下一个词的概率分布 <code>[vocab_size]</code>维向量</td>
</tr>
<tr>
<td style="text-align:left"><strong>速度</strong></td>
<td style="text-align:left"><strong>快</strong>（并行计算）</td>
<td style="text-align:left"><strong>慢</strong>（串行计算）</td>
</tr>
<tr>
<td style="text-align:left"><strong>关键</strong></td>
<td style="text-align:left">使用<strong>掩码</strong>和<strong>教师强制</strong></td>
<td style="text-align:left">使用<strong>自回归</strong>循环生成</td>
</tr>
</tbody>
</table>
</div>
<p><strong>掩码自注意力机制 的 输入输出 和 缩放点积自注意力 是一样的</strong>，唯一区别就是在计算得到的Q*K^T上面，加了掩码，（计算softmax之前）。</p>
<p><strong>交叉自注意力机制：</strong></p>
<p>当前Decoder块的Q用上一个Decoder块的输出<script type="math/tex">Z_{M*d}</script>计算得到。</p>
<p>当前Decoder块的K V用C计算得到。</p>
<h4 id="什么是多头，以及其他网络结构细节"><a href="#什么是多头，以及其他网络结构细节" class="headerlink" title="什么是多头，以及其他网络结构细节"></a>什么是多头，以及其他网络结构细节</h4><p><img src="https://pic2.zhimg.com/v2-f6380627207ff4d1e72addfafeaff0bb_r.jpg" alt="img"></p>
<h4 id="多头注意力的意思："><a href="#多头注意力的意思：" class="headerlink" title="多头注意力的意思："></a>多头注意力的意思：</h4><p>一个head=一个（Q,K,V） </p>
<p><strong>多头=多个并行的（Q,K,V）</strong></p>
<h5 id="单头的情况："><a href="#单头的情况：" class="headerlink" title="单头的情况："></a>单头的情况：</h5><p>有<script type="math/tex">M</script>个特征（或者单词）   （又叫seq_len: 句子长度,即单词数量）</p>
<p>每个单词用一个$d$维向量表示，记作矩阵<script type="math/tex">X_{M*d}</script>，矩阵有$M$行，<strong>每行对应一个特征/单词</strong>，每行是$d$维。</p>
<p>Self-Attention 的输入是矩阵$X$，则可以使用线性变换矩阵$W_Q,W_K,W_V$计算得到$Q,K,V$矩阵。</p>
<p>$W_Q,W_K,W_V$的维度是<script type="math/tex">d*d</script>，得到的$Q,K,V$矩阵的维度是<script type="math/tex">M*d</script>。 <strong>$X, Q, K, V$的每一行都对应一个单词。</strong></p>
<p>矩阵乘法：</p>
<script type="math/tex; mode=display">Q=X×W_Q</script><p>$K=X×W_K$</p>
<p>$V=X×W_V$</p>
<p>然后计算$Score(Q,K^T)$，即矩阵相乘$QK^T$，维度是$M*M$，元素$a_{ij}$代表第$i$个单词和第$j$个单词之间的关联程度（attention强度/权重/attention系数/相似性）。</p>
<p>（为了防止$a_{ij}$过大，所有的$a_{ij}$都要除以$\sqrt{d}$）</p>
<p>再对$QK^T$的每一行做$softmax$，使得每行各元素之和都为1.</p>
<p>得到新的维度为$M*M$的矩阵$softmax(QK^T)$。</p>
<p>再和矩阵$V$进行元素相乘，得到最终的输出矩阵$Z$，$Z=softmax(QK^T) × V$       </p>
<p>$Z$的维度是$M*d$</p>
<h5 id="多头的情况：【变化：单个注意力机制-→-多头注意力机制】"><a href="#多头的情况：【变化：单个注意力机制-→-多头注意力机制】" class="headerlink" title="多头的情况：【变化：单个注意力机制 → 多头注意力机制】"></a>多头的情况：【变化：单个注意力机制 → 多头注意力机制】</h5><p>将原始的 <code>d</code> 维特征空间分割成 <code>h</code> 个头，这 <code>h</code> 个头可以完全并行地计算自注意力。</p>
<p>注意：<strong>不是直接对输入矩阵 X 进行划分！！并不是划分输入 <code>X</code>（比如把 <code>(M, d)</code> 的矩阵切成 <code>h</code> 个 <code>(M, d/h)</code> 的矩阵）</strong>。</p>
<p><strong>而是把完整的X ∈R^[M,d] 都输入到并行的h个头中。</strong></p>
<p>比如有h个头，是并行的，即h组（Q,K,V）：</p>
<p>对于每一个头 <code>i</code> (i from 1 to h)，我们都有其对应的查询（Q）、键（K）、值（V）的线性变换投影矩阵：</p>
<ul>
<li><script type="math/tex; mode=display">W_Q^i ∈ ℝ^(d × d_Q)</script></li>
<li><script type="math/tex; mode=display">W_K^i ∈ ℝ^(d × d_K)</script></li>
<li><script type="math/tex; mode=display">W_V^i ∈ ℝ^(d × d_V)</script></li>
</ul>
<p>对于每个头，我们独立地进行线性投影：</p>
<ul>
<li><script type="math/tex; mode=display">Q^i = X · W_Q^i → 维度: (M, d_Q)</script></li>
<li><script type="math/tex; mode=display">K^i = X · W_K^i → 维度: (M, d_K)</script></li>
<li><script type="math/tex; mode=display">V^i = X · W_V^i → 维度: (M, d_V)</script></li>
</ul>
<p><strong>注意：有<script type="math/tex">d_Q=d_K=d_V=d/h</script></strong></p>
<p>比如d=512，有h=8个头，则每个头内部的<script type="math/tex">d_Q=d_K=d_V=512/8=64</script></p>
<p>之后一步步计算，每个头的输出$Z^i$的维度是<script type="math/tex">M*d_V</script></p>
<p>h个头，得到h个维度为<script type="math/tex">M*d_V</script>的<script type="math/tex">Z^i</script>，接下来：</p>
<p>①将这 <code>h</code> 个头的输出 <strong>拼接</strong> 起来。（<script type="math/tex">d_V*h=d</script>）</p>
<p>h个<script type="math/tex">Z^i</script>拼接，得到最终的Z，维度为<script type="math/tex">M*d</script></p>
<p>②线性投影：</p>
<p>将拼接后的结果 <code>Z</code> 通过一个<strong>可学习的线性变换</strong> <code>W^O</code></p>
<ul>
<li><strong>操作</strong>： <code>Output = Z · W^O</code></li>
<li><strong>参数矩阵 <code>W^O</code> 的维度</strong>： <code>(h * d_v, d)</code> 也就是 <code>(d, d)</code>（因为 <code>h * d_v = d</code>）。</li>
<li><strong>输出维度</strong>： <code>(M, d) · (d, d) = (M, d)</code></li>
</ul>
<p><strong>这一步的目的是让模型自由地学习和融合来自所有头的信息。</strong></p>
<p>最终，该多头注意力的输出是<script type="math/tex">MultiHeadOutput_{M*d}</script>，维度和输入<script type="math/tex">X_{M*d}</script>是一样的。</p>
<p>（<script type="math/tex">MultiHeadOutput_{M*d}</script>就是我上文【三种注意力机制（Encoder一种，Decoder两种）】中的<script type="math/tex">Z_{M*d}</script>）</p>
<h5 id="注意力后的Feed-Forward层："><a href="#注意力后的Feed-Forward层：" class="headerlink" title="注意力后的Feed Forward层："></a>注意力后的Feed Forward层：</h5><p>FFN对注意力层的输出 <code>Z</code>（维度 <code>(M, d)</code>）进行两步操作。</p>
<p>输出是<script type="math/tex">FFN(Z) = max(0, Z W_1 + b_1) W_2 + b_2</script>。</p>
<p>FFN(Z) 的维度仍然是 <code>(M, d)</code></p>
<ul>
<li><code>W_1</code> 是一个 <code>(d, d_ff)</code> 的矩阵，<code>b_1</code> 是维度 <code>d_ff</code> 的向量。</li>
<li><code>W_2</code> 是一个 <code>(d_ff, d)</code> 的矩阵，<code>b_2</code> 是维度 <code>d</code> 的向量。</li>
</ul>
<p><strong>d_ff &gt; d，即先升维再降维。</strong></p>
<p><em>ReLU激活函数 (<code>max(0, x)</code>) 提供了至关重要的非线性变换能力。</em></p>
<p><strong>FFN就是一个“升维 → 非线性激活 → 降维”的处理过程。</strong></p>
<h5 id="注意力后面的Add-amp-Norm-和Feed-Forward后面的Add-amp-Norm："><a href="#注意力后面的Add-amp-Norm-和Feed-Forward后面的Add-amp-Norm：" class="headerlink" title="注意力后面的Add &amp; Norm 和Feed Forward后面的Add &amp; Norm："></a>注意力后面的Add &amp; Norm 和Feed Forward后面的Add &amp; Norm：</h5><p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p>
<p><img src="https://pic1.zhimg.com/v2-a4b35db50f882522ee52f61ddd411a5a_1440w.jpg" alt="img"></p>
<p>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) <strong>表示注意力机制，和FFN的输出 (输出与输入 X 维度是一样的，都是M*d，所以可以相加)。</strong></p>
<p><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到。</p>
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构。</p>
<h1 id="Transformer其他细节："><a href="#Transformer其他细节：" class="headerlink" title="Transformer其他细节："></a>Transformer其他细节：</h1><p>见下面，写的易懂，要多看！</p>
<p><a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/338817680</a></p>
<p>补充笔记：</p>
<p>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding！</p>
<p>$PE_{pos,2i}，PE_{pos,2i+1}$，这里的$2i,2i+1$是维度索引，即分量，是在计算奇数编号和偶数编号的<strong>分量</strong>的值。</p>
<h5 id="层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别："><a href="#层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别：" class="headerlink" title="层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别："></a>层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别：</h5><p>假设输入矩阵为$X_{n*d}$，</p>
<p>n 是批量大小（batch size），代表有n个单词/物品，<strong>一行对应一个</strong>；</p>
<p>d 是每个单词/物品的特征数量。</p>
<p>Layer Normalization 的步骤如下：</p>
<ol>
<li><p><strong>计算均值</strong>： <strong>对每个样本（行）</strong>计算特征的均值。对于第$i$个样本$\mathbf{x}_i = (x_{i1}, x_{i2}, …, x_{id})$，均值是：</p>
<script type="math/tex; mode=display">
\mu_i = \frac{1}{d} \sum_{j=1}^{d} x_{ij}</script></li>
<li><p><strong>计算方差</strong>： 对每个样本计算特征的方差：</p>
<script type="math/tex; mode=display">
\sigma_i^2 = \frac{1}{d} \sum_{j=1}^{d} (x_{ij} - \mu_i)^2</script></li>
<li><p><strong>归一化</strong>： 使用均值和方差对每个特征进行归一化：</p>
<script type="math/tex; mode=display">
\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}</script><p>其中 $\epsilon$ 是一个非常小的常数，用于防止除零错误。</p>
</li>
<li><p><strong>缩放和平移</strong>： 在归一化后，Layer Normalization 会引入两个可学习的参数 $\gamma$ 和 $\beta$，分别用于缩放和平移。最终输出是：</p>
<script type="math/tex; mode=display">
y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j</script><p>其中 $\gamma_j$ 和 $\beta_j$是为每个特征（每个列）学习到的参数。</p>
</li>
</ol>
<p>总结：</p>
<p>Layer Normalization 对每个样本的特征进行归一化。</p>
<p>它不像 Batch Normalization 依赖于批量数$n$，而是<strong>对每个样本单独归一化</strong>，适用于处理序列数据（如RNN、Transformer等），尤其在批量大小较小或单个样本处理时。</p>
<p>批归一化的步骤：</p>
<ol>
<li><p><strong>计算批次均值</strong>： 对整个批次的<strong>每个特征维度（列）</strong>计算均值，即计算<strong>每一列（特征）在批次中所有样本的均值</strong>。对于第$j$个特征维度，它的批次均值是：</p>
<script type="math/tex; mode=display">
\mu_j = \frac{1}{n} \sum_{i=1}^{n} x_{ij}</script><p>其中，$x_{ij}$是第 $i$个样本在第$j$个特征维度上的值。</p>
</li>
<li><p><strong>计算批次方差</strong>： 对整个批次的每个特征维度（列）计算方差，即计算每一列（特征）在批次中所有样本的方差。对于第$j$个特征维度，它的批次方差是：</p>
<script type="math/tex; mode=display">
\sigma_j^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{ij} - \mu_j)^2</script><p>这里的$\sigma_j^2$是第$j$个特征维度在批次中的方差。</p>
</li>
<li><p><strong>归一化</strong>： 使用计算得到的均值$\mu_j$和方差 $\sigma_j^2$对每个样本的每个特征进行归一化处理。对于第$i$个样本在第$j$个特征维度上的归一化值：</p>
<script type="math/tex; mode=display">
\hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}</script><p>这里$\epsilon$ 是一个很小的常数，用来防止方差为零时出现除零错误。</p>
</li>
<li><p><strong>缩放和平移（可选）</strong>： 批归一化通常会引入两个可学习的参数：$\gamma_j$和$\beta_j$，分别用于缩放和偏移归一化后的值。这样做的目的是让网络有更大的表达能力，因为在训练过程中，经过归一化的输出可以通过这些参数重新调整回原始尺度。</p>
<script type="math/tex; mode=display">
y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j</script><p>其中，$\gamma_j$是缩放因子，$\beta_j$是偏移因子，通常会初始化为1和0。</p>
</li>
</ol>
<p>批归一化通常在卷积神经网络（CNN）和全连接层网络（FCN）中使用。</p>
<p><strong>层归一化</strong>：是在每个样本的特征维度上进行归一化，每个样本独立归一化。<strong>每行进行归一化、</strong></p>
<p><strong>批归一化</strong>：是在整个批次上对每个特征维度进行归一化，考虑了批次中的所有样本。<strong>每列进行归一化。</strong></p>
<p>每个Decoder Block：</p>
<ul>
<li>包含两个 Multi-Head Attention 层。</li>
<li>第一个 Multi-Head Attention 层采用了 Masked 操作：<ul>
<li>在计算得到$QK^T$，对$QK^T$的每一行做$softmax$之前，</li>
<li>先使用<strong>Mask</strong>矩阵，将$QK^T$变成<strong>Mask</strong> $QK^T$，</li>
<li>再对<strong>Mask</strong> $QK^T$的每一行做$softmax$。</li>
</ul>
</li>
<li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算得出，而<strong>Q</strong>使用上一个 Decoder block 的输出计算得出。</li>
</ul>
<p>Encoder和Decoder的输入输出：</p>
<p>Encoder的输入：源文本的单词表示矩阵$X_{n,d}$，  如“我 有 一只 猫”，共4个单词</p>
<p>Encoder的输出：信息编码矩阵$C_{n,d}$，<strong>与$X$的维度一致</strong>，<script type="math/tex">（n=seq\_len,d=emd\_size）</script></p>
<p>Decoder的输入：目标文本的单词表示矩阵$X_{n,d}$，如”&lt;Begin&gt; I have a cat”，共5个单词</p>
<p>（Encoder的输出矩阵$C$会输入到Decoder的第二个$Multi-Head \ Attention$中）</p>
<p>Decoder的输出：预测的单词序列，如“I have a cat &lt;end&gt;”。</p>


        <hr />
        <!-- Pager -->
        <ul class="pager">
           
          <li class="next">
            <a
              href="/2024/12/09/博客搭建记录-20241208/"
              data-toggle="tooltip"
              data-placement="top"
              title="博客搭建记录_20241208"
              >Next Post &rarr;</a
            >
          </li>
          
        </ul>

        <!-- tip start -->
         
        <div class="comment_notes">
          <p>Written by Dowerinne | All Rights Reserved</p>
        </div>
        
        <!-- tip end -->

        <!-- Music start-->
        
        <!-- Music end -->

        <!-- Sharing -->
        
        <div
          class="social-share"
          data-wechat-qrcode-helper=""
          align="center"
        ></div>
        <!--  css & js -->
        <link
          rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"
        />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
        
        <!-- Sharing -->

        <!-- gitment start -->
        
        <!-- gitment end -->

        <!-- 来必力City版安装代码 -->
        
        <!-- City版安装代码已完成 -->

        <!-- disqus comment start -->
        
        <!-- disqus comment end -->

        <!-- waline comment start -->
        
        <!-- waline comment end -->
      </div>

      <!-- Tabe of Content -->
      <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#多头自注意力机制与Transformer"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">多头自注意力机制与Transformer</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）"><span class="toc-nav-number">1.0.0.1.</span> <span class="toc-nav-text">自注意力机制（计算过程见：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;338817680 3.2节与3.3节）</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#多头自注意力机制"><span class="toc-nav-number">1.0.0.2.</span> <span class="toc-nav-text">多头自注意力机制</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#三种注意力机制（Encoder一种，Decoder两种）"><span class="toc-nav-number">1.0.0.3.</span> <span class="toc-nav-text">三种注意力机制（Encoder一种，Decoder两种）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#后续过程："><span class="toc-nav-number">1.0.0.3.1.</span> <span class="toc-nav-text">后续过程：</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#什么是多头，以及其他网络结构细节"><span class="toc-nav-number">1.0.0.4.</span> <span class="toc-nav-text">什么是多头，以及其他网络结构细节</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#多头注意力的意思："><span class="toc-nav-number">1.0.0.5.</span> <span class="toc-nav-text">多头注意力的意思：</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#单头的情况："><span class="toc-nav-number">1.0.0.5.1.</span> <span class="toc-nav-text">单头的情况：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#多头的情况：【变化：单个注意力机制-→-多头注意力机制】"><span class="toc-nav-number">1.0.0.5.2.</span> <span class="toc-nav-text">多头的情况：【变化：单个注意力机制 → 多头注意力机制】</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#注意力后的Feed-Forward层："><span class="toc-nav-number">1.0.0.5.3.</span> <span class="toc-nav-text">注意力后的Feed Forward层：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#注意力后面的Add-amp-Norm-和Feed-Forward后面的Add-amp-Norm："><span class="toc-nav-number">1.0.0.5.4.</span> <span class="toc-nav-text">注意力后面的Add &amp; Norm 和Feed Forward后面的Add &amp; Norm：</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#Transformer其他细节："><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Transformer其他细节：</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别："><span class="toc-nav-number">2.0.0.0.1.</span> <span class="toc-nav-text">层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别：</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
            
          
          </div>
        </aside>
      
    


      <!-- Sidebar Container -->
      <div
        class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 sidebar-container"
      >
        <!-- Featured Tags -->
        
        <section>
          <!-- no hr -->
          <h5><a href="/tags/">FEATURED TAGS</a></h5>
          <div class="tags">
            
          </div>
        </section>
        

        <!-- Friends Blog -->
        
      </div>
    </div>
  </div>
</article>

 
<!-- async load function -->
<script>
  function async(u, c) {
    var d = document,
      t = "script",
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener(
        "load",
        function (e) {
          c(null, e);
        },
        false
      );
    }
    s.parentNode.insertBefore(o, s);
  }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
  async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js", function () {
    anchors.options = {
      visible: "hover",
      placement: "left",
      icon: "ℬ",
    };
    anchors
      .add()
      .remove(".intro-header h1")
      .remove(".subheading")
      .remove(".sidebar-container h5");
  });
</script>

<style type="text/css">
  /* place left on bigger screen */
  @media all and (min-width: 800px) {
    .anchorjs-link {
      position: absolute;
      left: -0.75em;
      font-size: 1.1em;
      margin-top: -0.1em;
    }
  }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Dowerinne 2025 
                    <br>
                    Powered by 
                    <a href="https://github.com/Dowerinne/Dowerinne.github.io" target="_blank" rel="noopener">
                        <i>Dowerinne</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=Dowerinne&repo=Dowerinne.github.io&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://dowerinne.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🎄&quot;,&quot;🎇&quot;,&quot;🎉&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>

</html>
