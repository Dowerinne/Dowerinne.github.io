<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="google-site-verification"
    content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI"
  />
  <meta name="baidu-site-verification" content="093lY4ziMu" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, user-scalable=no"
  />
  <meta name="description" content="A hexo theme" />
  <meta name="keyword" content="hexo-theme-snail" />
  <link rel="shortcut icon" href="/img/ironman-draw.png" />
  <link
    rel="stylesheet"
    href="https://unpkg.com/@waline/client@v3/dist/waline.css"
  />
  <link rel="stylesheet" href="/css/walineRoot.css" />
  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
  <title>
     Transformer和多头注意力机制 - Dowerinne&#39;s blog 
  </title>

  <link
    rel="canonical"
    href="https://dusign.net/2024/12/10/Transformer和多头注意力机制/"
  />

  <!-- Bootstrap Core CSS -->
  
<link rel="stylesheet" href="/css/bootstrap.min.css">


  <!-- Custom CSS -->
   
<link rel="stylesheet" href="/css/dusign-light.css">
 
<link rel="stylesheet" href="/css/dusign-common-light.css">
 
<link rel="stylesheet" href="/css/font-awesome.css">
 
<link rel="stylesheet" href="/css/toc.css">

  <!-- background effects end -->
  

  <!-- Pygments Highlight CSS -->
  
<link rel="stylesheet" href="/css/highlight.css">
 
<link rel="stylesheet" href="/css/widget.css">
 
<link rel="stylesheet" href="/css/rocket.css">

  
<link rel="stylesheet" href="/css/signature.css">
 
<link rel="stylesheet" href="/css/fonts.googleapis.css">


  <link
    rel="stylesheet"
    href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"
  />

  <!-- photography -->
  
<link rel="stylesheet" href="/css/photography.css">


  <!-- ga & ba script hoook -->
  <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                        </div>
                        <h1>Transformer和多头注意力机制</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Dowerinne on
                            2024-12-10
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">1.9k</span> and
                                Reading Time <span class="post-count">7</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Dowerinne&#39;s blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/photography/">Photography</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Post Content -->
<article>
  <div class="container">
    <div class="row">
      <!-- Post Container -->
      <div
        class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 post-container"
      >
        <h1 id="多头自注意力机制与Transformer"><a href="#多头自注意力机制与Transformer" class="headerlink" title="多头自注意力机制与Transformer"></a>多头自注意力机制与Transformer</h1><p>2024.12.06整理</p>
<h4 id="自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）"><a href="#自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）" class="headerlink" title="自注意力机制（计算过程见：https://zhuanlan.zhihu.com/p/338817680 3.2节与3.3节）"></a>自注意力机制（计算过程见：<a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/338817680</a> 3.2节与3.3节）</h4><p>有$M$个特征（或者单词）   （又叫seq_len: 句子长度,即单词数量）</p>
<p>每个单词用一个$d$维向量表示，记作矩阵<script type="math/tex">X_{M*d}</script>，矩阵有$M$行，<strong>每行对应一个特征/单词</strong>，每行是$d$维。</p>
<p>Self-Attention 的输入是矩阵$X$，则可以使用线性变换矩阵$W_Q,W_K,W_V$计算得到$Q,K,V$矩阵。</p>
<p>$W_Q,W_K,W_V$的维度是<script type="math/tex">d*d^{\prime}</script>，得到的$Q,K,V$矩阵的维度是<script type="math/tex">M*d'</script>。 <strong>$X, Q, K, V$的每一行都对应一个单词。</strong></p>
<p>矩阵乘法：</p>
<p>$Q=X×W_Q$</p>
<p>$K=X×W_K$</p>
<p>$V=X×W_V$</p>
<p>然后计算$Score(Q,K^T)$，即矩阵相乘$QK^T$，维度是$M*M$，元素$a_{ij}$代表第$i$个单词和第$j$个单词之间的关联程度（attention强度/权重/attention系数/相似性）。</p>
<p>（为了防止$a_{ij}$过大，所有的$a_{ij}$都要除以$\sqrt{d’}$）</p>
<p>再对$QK^T$的每一行做$softmax$，使得每行各元素之和都为1.</p>
<p>得到新的维度为$M*M$的矩阵$softmax(QK^T)$。</p>
<p>再和矩阵$V$进行元素相乘，得到最终的输出矩阵$Z$，$Z=softmax(QK^T) × V$       </p>
<p>$Z$的维度是$M*d’$</p>
<p><strong>矩阵$X,V,Z$的每一行都是对每个单词的向量表示，都有$M$行，因此该过程简单来讲就是$X→V→Z$，</strong></p>
<p>得到最终的矩阵$Z$，<font color='red'>$Z$的每行是作为对每个单词的新向量表示。</font></p>
<font color='red'>每个单词的新向量，都是融合了其它单词信息的新向量。而$X$的每行是没有融合其他单词信息的旧向量。</font>

<p>计算公式写成：</p>
<p>自注意力机制的输出为矩阵$Z$：</p>
<script type="math/tex; mode=display">
Z=Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d'}})V</script><h4 id="多头自注意力机制"><a href="#多头自注意力机制" class="headerlink" title="多头自注意力机制"></a>多头自注意力机制</h4><p>（一组头：就是一组$Q \ K \ V$矩阵，多头有多组$Q \ K \ V$矩阵，从而得到不同的输出矩阵$Z_i$）</p>
<p>【不同的头又叫不同的“子空间”，<strong>头=子空间</strong>，可理解为不同的<strong>角度</strong>】</p>
<p>【如果想从多个角度看，可以用多个头，即换不同的矩阵$W_Q,W_K,W_V$得到不同的$Q,K,V$，然后得到不同的输出矩阵$Z_i$】</p>
<p>多头自注意力机制<strong>Multi-Head Attention</strong>，是由多个自注意力机制<strong>Self-Attention</strong>组成的。</p>
<p>Multi-Head Attention 包含多个 Self-Attention 层，</p>
<p>首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>$Z_i$</strong>。</p>
<p>比如当 h=8 时，此时会得到 8 个输出矩阵<strong>$Z_i$</strong>。</p>
<p>得到 8 个输出矩阵$Z_1$~$Z_8$之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p>
<p>（每个$Z_i$的维度是<script type="math/tex">M*d'</script>，与$X$的维度不同；<strong>而多头自注意力机制的最终输出$Z$的维度是<script type="math/tex">M*d</script>，与$X$的维度相同</strong>。）</p>
<h1 id="Transformer其他细节："><a href="#Transformer其他细节：" class="headerlink" title="Transformer其他细节："></a>Transformer其他细节：</h1><p>见下面，写的易懂，要多看！</p>
<p><a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/338817680</a></p>
<p>补充笔记：</p>
<p>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding！</p>
<p>$PE_{pos,2i}，PE_{pos,2i+1}$，这里的$2i,2i+1$是维度索引，即分量，是在计算奇数编号和偶数编号的<strong>分量</strong>的值。</p>
<h5 id="层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别："><a href="#层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别：" class="headerlink" title="层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别："></a>层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别：</h5><p>假设输入矩阵为$X_{n*d}$，</p>
<p>n 是批量大小（batch size），代表有n个单词/物品，<strong>一行对应一个</strong>；</p>
<p>d 是每个单词/物品的特征数量。</p>
<p>Layer Normalization 的步骤如下：</p>
<ol>
<li><p><strong>计算均值</strong>： <strong>对每个样本（行）</strong>计算特征的均值。对于第$i$个样本$\mathbf{x}_i = (x_{i1}, x_{i2}, …, x_{id})$，均值是：</p>
<script type="math/tex; mode=display">
\mu_i = \frac{1}{d} \sum_{j=1}^{d} x_{ij}</script></li>
<li><p><strong>计算方差</strong>： 对每个样本计算特征的方差：</p>
<script type="math/tex; mode=display">
\sigma_i^2 = \frac{1}{d} \sum_{j=1}^{d} (x_{ij} - \mu_i)^2</script></li>
<li><p><strong>归一化</strong>： 使用均值和方差对每个特征进行归一化：</p>
<script type="math/tex; mode=display">
\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}</script><p>其中 $\epsilon$ 是一个非常小的常数，用于防止除零错误。</p>
</li>
<li><p><strong>缩放和平移</strong>： 在归一化后，Layer Normalization 会引入两个可学习的参数 $\gamma$ 和 $\beta$，分别用于缩放和平移。最终输出是：</p>
<script type="math/tex; mode=display">
y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j</script><p>其中 $\gamma_j$ 和 $\beta_j$是为每个特征（每个列）学习到的参数。</p>
</li>
</ol>
<p>总结：</p>
<p>Layer Normalization 对每个样本的特征进行归一化。</p>
<p>它不像 Batch Normalization 依赖于批量数$n$，而是<strong>对每个样本单独归一化</strong>，适用于处理序列数据（如RNN、Transformer等），尤其在批量大小较小或单个样本处理时。</p>
<p>批归一化的步骤：</p>
<ol>
<li><p><strong>计算批次均值</strong>： 对整个批次的<strong>每个特征维度（列）</strong>计算均值，即计算<strong>每一列（特征）在批次中所有样本的均值</strong>。对于第$j$个特征维度，它的批次均值是：</p>
<script type="math/tex; mode=display">
\mu_j = \frac{1}{n} \sum_{i=1}^{n} x_{ij}</script><p>其中，$x_{ij}$是第 $i$个样本在第$j$个特征维度上的值。</p>
</li>
<li><p><strong>计算批次方差</strong>： 对整个批次的每个特征维度（列）计算方差，即计算每一列（特征）在批次中所有样本的方差。对于第$j$个特征维度，它的批次方差是：</p>
<script type="math/tex; mode=display">
\sigma_j^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{ij} - \mu_j)^2</script><p>这里的$\sigma_j^2$是第$j$个特征维度在批次中的方差。</p>
</li>
<li><p><strong>归一化</strong>： 使用计算得到的均值$\mu_j$和方差 $\sigma_j^2$对每个样本的每个特征进行归一化处理。对于第$i$个样本在第$j$个特征维度上的归一化值：</p>
<script type="math/tex; mode=display">
\hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}</script><p>这里$\epsilon$ 是一个很小的常数，用来防止方差为零时出现除零错误。</p>
</li>
<li><p><strong>缩放和平移（可选）</strong>： 批归一化通常会引入两个可学习的参数：$\gamma_j$和$\beta_j$，分别用于缩放和偏移归一化后的值。这样做的目的是让网络有更大的表达能力，因为在训练过程中，经过归一化的输出可以通过这些参数重新调整回原始尺度。</p>
<script type="math/tex; mode=display">
y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j</script><p>其中，$\gamma_j$是缩放因子，$\beta_j$是偏移因子，通常会初始化为1和0。</p>
</li>
</ol>
<p>批归一化通常在卷积神经网络（CNN）和全连接层网络（FCN）中使用。</p>
<p><strong>层归一化</strong>：是在每个样本的特征维度上进行归一化，每个样本独立归一化。<strong>每行进行归一化、</strong></p>
<p><strong>批归一化</strong>：是在整个批次上对每个特征维度进行归一化，考虑了批次中的所有样本。<strong>每列进行归一化。</strong></p>
<p>每个Decoder Block：</p>
<ul>
<li>包含两个 Multi-Head Attention 层。</li>
<li>第一个 Multi-Head Attention 层采用了 Masked 操作：<ul>
<li>在计算得到$QK^T$，对$QK^T$的每一行做$softmax$之前，</li>
<li>先使用<strong>Mask</strong>矩阵，将$QK^T$变成<strong>Mask</strong> $QK^T$，</li>
<li>再对<strong>Mask</strong> $QK^T$的每一行做$softmax$。</li>
</ul>
</li>
<li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算得出，而<strong>Q</strong>使用上一个 Decoder block 的输出计算得出。</li>
</ul>
<p>Encoder和Decoder的输入输出：</p>
<p>Encoder的输入：源文本的单词表示矩阵$X_{n,d}$，  如“我 有 一只 猫”，共4个单词</p>
<p>Encoder的输出：信息编码矩阵$C_{n,d}$，<strong>与$X$的维度一致</strong>，<script type="math/tex">（n=seq\_len,d=emd\_size）</script></p>
<p>Decoder的输入：目标文本的单词表示矩阵$X_{n,d}$，如”&lt;Begin&gt; I have a cat”，共5个单词</p>
<p>（Encoder的输出矩阵$C$会输入到Decoder的第二个$Multi-Head \ Attention$中）</p>
<p>Decoder的输出：预测的单词序列，如“I have a cat &lt;end&gt;”。</p>


        <hr />
        <!-- Pager -->
        <ul class="pager">
           
          <li class="next">
            <a
              href="/2024/12/09/博客搭建记录-20241208/"
              data-toggle="tooltip"
              data-placement="top"
              title="博客搭建记录_20241208"
              >Next Post &rarr;</a
            >
          </li>
          
        </ul>

        <!-- tip start -->
         
        <div class="comment_notes">
          <p>This is copyright.</p>
        </div>
        
        <!-- tip end -->

        <!-- Music start-->
         
<link rel="stylesheet" href="/css/music-player/fonts/iconfont.css">


<link rel="stylesheet" href="/css/music-player/css/reset.css">


<link rel="stylesheet" href="/css/music-player/css/player.css">


<div class="music-player">
    <audio class="music-player__audio" ></audio>
    <div class="music-player__main">
        <div class="music-player__blur"></div>
        <div class="music-player__disc">
            <div class="music-player__image">
                <img width="100%" src="" alt="">
            </div>
            <div class="music-player__pointer"><img width="100%" src="/img/cd_tou.png" alt=""></div>
        </div>
        <div class="music-player__controls">
            <div class="music__info">
                <h3 class="music__info--title">...</h3>
                <p class="music__info--singer">...</p>
            </div>
            <div class="player-control">
                <div class="player-control__content">
                    <div class="player-control__btns">
                        <div class="player-control__btn player-control__btn--prev"><i class="iconfont icon-prev"></i></div>
                        <div class="player-control__btn player-control__btn--play"><i class="iconfont icon-play"></i></div>
                        <div class="player-control__btn player-control__btn--next"><i class="iconfont icon-next"></i></div>
                        <div class="player-control__btn player-control__btn--mode"><i class="iconfont icon-loop"></i></div>
                    </div>
                    <div class="player-control__volume">
                        <div class="control__volume--icon player-control__btn"><i class="iconfont icon-volume"></i></div>
                        <div class="control__volume--progress player_progress"></div>
                    </div>
                </div>
                <div class="player-control__content">
                    <div class="player__song--progress player_progress"></div>
                    <div class="player__song--timeProgess nowTime">00:00</div>
                    <div class="player__song--timeProgess totalTime">00:00</div>
                </div>
            </div>
        </div>
    </div>
</div>


<script src="/js/music-player/utill.js"></script>


<script src="/js/music-player/jquery.min.js"></script>

<!-- netease; qqkg -->
<!--
<script src="/js/music-player/player.js?library=config.music.library.js"></script>
-->
<script src="../../../../js/music-player/player.js?library=netease&music=https://music.163.com/#/song?id=487527981"></script> 
        <!-- Music end -->

        <!-- Sharing -->
        
        <div
          class="social-share"
          data-wechat-qrcode-helper=""
          align="center"
        ></div>
        <!--  css & js -->
        <link
          rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"
        />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
        
        <!-- Sharing -->

        <!-- gitment start -->
        
        <!-- gitment end -->

        <!-- 来必力City版安装代码 -->
        
        <!-- City版安装代码已完成 -->

        <!-- disqus comment start -->
        
        <!-- disqus comment end -->

        <!-- waline comment start -->
        
        <!-- waline comment end -->
      </div>

      <!-- Tabe of Content -->
      <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#多头自注意力机制与Transformer"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">多头自注意力机制与Transformer</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#自注意力机制（计算过程见：https-zhuanlan-zhihu-com-p-338817680-3-2节与3-3节）"><span class="toc-nav-number">1.0.0.1.</span> <span class="toc-nav-text">自注意力机制（计算过程见：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;338817680 3.2节与3.3节）</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#多头自注意力机制"><span class="toc-nav-number">1.0.0.2.</span> <span class="toc-nav-text">多头自注意力机制</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#Transformer其他细节："><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Transformer其他细节：</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#层归一化（Layer-Normalization）和批归一化（Batch-Normalization，BN）的区别："><span class="toc-nav-number">2.0.0.0.1.</span> <span class="toc-nav-text">层归一化（Layer Normalization）和批归一化（Batch Normalization，BN）的区别：</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
            
          
          </div>
        </aside>
      
    


      <!-- Sidebar Container -->
      <div
        class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 sidebar-container"
      >
        <!-- Featured Tags -->
        
        <section>
          <!-- no hr -->
          <h5><a href="/tags/">FEATURED TAGS</a></h5>
          <div class="tags">
            
          </div>
        </section>
        

        <!-- Friends Blog -->
        
        <hr />
        <h5>FRIENDS</h5>
        <ul class="list-inline">
          
          <li>
            <a href="https://blog.csdn.net/d_Nail" target="_blank">Dusign&#39;s Blog</a>
          </li>
          
          <li>
            <a href="#" target="_blank">Dusign&#39;s Web</a>
          </li>
          
          <li>
            <a href="https://github.com/dusign" target="_blank">Dusign&#39;s Github</a>
          </li>
          
          <li>
            <a href="#" target="_blank">Other</a>
          </li>
          
        </ul>
        
      </div>
    </div>
  </div>
</article>

 
<!-- async load function -->
<script>
  function async(u, c) {
    var d = document,
      t = "script",
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener(
        "load",
        function (e) {
          c(null, e);
        },
        false
      );
    }
    s.parentNode.insertBefore(o, s);
  }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
  async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js", function () {
    anchors.options = {
      visible: "hover",
      placement: "left",
      icon: "ℬ",
    };
    anchors
      .add()
      .remove(".intro-header h1")
      .remove(".subheading")
      .remove(".sidebar-container h5");
  });
</script>

<style type="text/css">
  /* place left on bigger screen */
  @media all and (min-width: 800px) {
    .anchorjs-link {
      position: absolute;
      left: -0.75em;
      font-size: 1.1em;
      margin-top: -0.1em;
    }
  }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Dowerinne 2024 
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=dusign&repo=hexo-theme-snail&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://dusign.net/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>

</html>
